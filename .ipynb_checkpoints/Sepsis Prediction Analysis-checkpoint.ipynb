{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a57296d5",
   "metadata": {},
   "source": [
    "# Sepsis Prediction Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54fff36",
   "metadata": {},
   "source": [
    "### Business Understanding\n",
    "\n",
    "Sepsis is a critical medical condition characterized by the body's extreme response to infection, often leading to severe tissue damage, multiple organ failure, and even death. Each year, approximately 30 million individuals worldwide develop sepsis, with a staggering one-fifth of them succumbing to the disease. Detecting sepsis early and initiating immediate treatment is crucial for saving lives and improving patient outcomes. This project aims to leverage machine learning to predict whether patients will develop sepsis using their physiological data.\n",
    "\n",
    "Project Objectives:\n",
    "The primary objectives of this project are as follows:\n",
    "1. Early Sepsis Detection: Develop a robust machine learning model capable of accurately predicting whether patients will develop sepsis using their physiological data.\n",
    "2. Life-Saving Potential: By accurately predicting whether patients will develop sepsis, this project aims to enable healthcare professionals to intervene promptly, potentially saving lives and reducing the severity of complications associated with sepsis.\n",
    "3. Model Integration: Implement the trained machine learning model into a user-friendly FastAPI-based application. This integration will make the model accessible to multiple healthcare professionals, streamlining the prediction process and facilitating early sepsis diagnosis.\n",
    "\n",
    "Data Source:\n",
    "The project relies on test and train datasets from a modified version of a publicly available patients data source. These train and test datasets are publicly available on Kaggle.\n",
    "\n",
    "Data Preprocessing:\n",
    "Data preprocessing will play a crucial role in this project. Preprocessing steps include feature scaling, and dataset  balancing, and data splitting to improve model performance.\n",
    "\n",
    "Machine Learning Models:\n",
    "Various machine learning algorithms, such as logistic regression, random forests, support vector machines, and neural networks, will be explored to determine the most effective model for sepsis prediction. Model selection will be based on factors like accuracy, sensitivity, specificity, and interpretability.\n",
    "\n",
    "Model Evaluation:\n",
    "The performance of the developed models will be rigorously evaluated using appropriate metrics, including but not limited to precision, recall, F1 score, and area under the receiver operating characteristic curve (AUC-ROC). Cross-validation techniques will help ensure the model's generalizability.\n",
    "\n",
    "FastAPI Integration:\n",
    "To make the sepsis prediction model accessible to healthcare professionals, it will be integrated into a FastAPI-based web application. This integration will provide a user-friendly interface where users can input patient data and receive predictions in real-time.\n",
    "\n",
    "Project Impact:\n",
    "Successful implementation of this project can have a profound impact on healthcare outcomes. Early sepsis detection can lead to faster intervention, reduced mortality rates, and improved patient recovery. Additionally, by making the model available through FastAPI, it becomes a valuable tool for healthcare providers worldwide, potentially saving countless lives.\n",
    "\n",
    "Conclusion:\n",
    "This Sepsis Prediction Project combines cutting-edge machine learning with user-friendly software integration to tackle a critical medical challenge. The goal is to provide healthcare professionals with a powerful tool for early sepsis detection, ultimately leading to better patient care and improved survival rates in the face of this life-threatening condition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4de91ce",
   "metadata": {},
   "source": [
    "### Hypothesis\n",
    "\n",
    "Null Hypothesis (H0):\n",
    "The machine learning model's accuracy in predicting sepsis based on patients' physiological data is not significantly different from a baseline level, suggesting that the model's predictions are no better than random chance.\n",
    "\n",
    "Alternative Hypothesis (H1):\n",
    "The machine learning model's accuracy in predicting sepsis based on patients' physiological data is significantly better than a baseline level, indicating that the model provides valuable predictive capabilities beyond random chance.\n",
    "\n",
    "In simpler terms:\n",
    "H0: The machine learning model doesn't improve sepsis prediction beyond random chance.\n",
    "H1: The machine learning model significantly improves sepsis prediction beyond random chance.\n",
    "\n",
    "This hypothesis specifically addresses the improvement in sepsis prediction and aligns with the project's objective of determining whether the model's predictions are meaningful compared to random guessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4949e55a",
   "metadata": {},
   "source": [
    "### Analytical Questions\n",
    "\n",
    "1. How many patients on the train dataset have developed sepsis?.\n",
    "2. Which age group has more occurence of sepsis?\n",
    "3. Does having a health insurance reduce the chances of patients developing sepsis?\n",
    "4. Does Body Mass Index (BMI) have a direct correlation with sepsis development?\n",
    "5. Does the blood parameters play a role in sepsis development?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5f9e10",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1381777f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the needed packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Libraries for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Library for testing the hypothesis\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Library for pandas profiling\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "# Library for splitting the train data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Library for feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Library for feature encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Libraries for balancing the dataset\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Libraries for modelling\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Libraries for model evaluation\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "# Library for hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Library for working with operating system\n",
    "import os\n",
    "\n",
    "# Library to serialize a Python object into a flat byte stream and transform a byte stream back into a Python object\n",
    "import pickle\n",
    "\n",
    "# Library to handle warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de99d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets.\n",
    "\n",
    "train = pd.read_csv('data/Paitients_Files_Train.csv')\n",
    "test = pd.read_csv('data/Paitients_Files_Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce66d7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first five rows of the train dataset\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af36a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first five rows of the test dataset\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c837fe",
   "metadata": {},
   "source": [
    "The train dataset has a 'Sepssis' column which is absent in the test dataset. This 'Sepssis' column will serve as the target column when training the model. This column will be renamed to 'Sepsis'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d856d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename 'Sepssis' column to 'Sepsis'\n",
    "\n",
    "train.rename(columns={'Sepssis': 'Sepsis'}, inplace=True)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a420e3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of rows and columns on both datasets.\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626d4848",
   "metadata": {},
   "source": [
    "The train dataset has 599 rows and 11 columns, while the test dataset has 169 rows and 10 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272118df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the datatypes and the presence of missing values on the train dataset.\n",
    "\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bba73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the datatypes and the presence of missing values on the test dataset.\n",
    "\n",
    "test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81fbc87",
   "metadata": {},
   "source": [
    "There are no empty cells in both the train and test dataset. And the datatype of each column in both datasets are consistent with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1521c1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm that both train and test datasets have no missing values\n",
    "\n",
    "train.isna().sum().sum(), test.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9816eefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for the presence of duplicates on the train and test datasets.\n",
    "\n",
    "train.duplicated().sum(), test.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e53f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigating the columns on the train dataset.\n",
    "\n",
    "train.columns\n",
    "for column in train.columns:\n",
    "    print('column: {} - unique value: {}'.format(column, train[column].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8fd5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the numerical columns of the train dataset\n",
    "train_num = train.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Obtain the numerical columns of the test dataset\n",
    "test_num = test.select_dtypes(include=['float64', 'int64']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def3ef03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the numerical values on the train dataset.\n",
    "\n",
    "train[train_num].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0985cbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the correlation of the numerical values on the train dataset.\n",
    "\n",
    "train[train_num].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9811c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the correlation with a heatmap\n",
    "\n",
    "sns.heatmap(train[train_num].corr(), annot=True)\n",
    "\n",
    "# Save the chart as an image file\n",
    "plt.savefig('Images/Correlation of the numerical columns of the train dataset.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b151daad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the numerical values on the test dataset.\n",
    "\n",
    "test[test_num].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047d2ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the correlation of the numerical values on the test dataset.\n",
    "\n",
    "test[test_num].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a44a432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the correlation with a heatmap\n",
    "\n",
    "sns.heatmap(test[test_num].corr(), annot=True)\n",
    "\n",
    "# Save the chart as an image file\n",
    "plt.savefig('Images/Correlation of the numerical columns of the test dataset.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aae91d7",
   "metadata": {},
   "source": [
    "### Answering Analytical Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf284c1",
   "metadata": {},
   "source": [
    "1. How many patients on the train dataset have developed sepsis?."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a97cd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of patients with and without sepsis on the train dataset\n",
    "sepsis_count = train['Sepsis'].value_counts()\n",
    "print(sepsis_count)\n",
    "\n",
    "# Create a bar chart\n",
    "plt.figure(figsize=(6, 4))\n",
    "sepsis_count.plot(kind='bar', color=['darkgreen', 'darkred'])\n",
    "plt.title('Number of Patients with and without Sepsis')\n",
    "plt.xlabel('Sepsis')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d42ca6",
   "metadata": {},
   "source": [
    "2. Which age group has more occurrence of sepsis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726d0aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram of age distribution for sepsis-positive and sepsis-negative patients\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(data=train, x='Age', hue='Sepsis', bins=20, kde=True)\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Age Distribution of Patients with and without Sepsis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0451a526",
   "metadata": {},
   "source": [
    "Does having health insurance reduce the chances of patients developing sepsis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5bf0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a countplot to compare sepsis occurrence with and without insurance\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(data=train, x='Insurance', hue='Sepsis')\n",
    "plt.xlabel('Insurance')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Sepsis Occurrence with and without Health Insurance')\n",
    "plt.xticks([0, 1], ['No Insurance', 'Has Insurance'])\n",
    "plt.legend(title='Sepsis', labels=['Negative', 'Positive'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc057a29",
   "metadata": {},
   "source": [
    "4. Does Body Mass Index (BMI) have a direct correlation with sepsis development?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9be1cf5",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "Feature engineering is the process that selects and transforms raw data from datasets into the desired features that can be used in supervised learning for modelling.\n",
    "\n",
    "In order to preserve the original cleaned datasets for future analysis, a copy of the train and test datasets will be created and used feature engineering.\n",
    "\n",
    "Also, in order to avoid data leakage the copy of the train dataset created will be splitted to obtain the training set and the validation set before feature engineering processes are carried out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fe1b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the train and test datasets on which to carry out the feature engineering processes\n",
    "\n",
    "train_data = train.copy()\n",
    "test_data = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8a0aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'ID' column as it is not needed in modelling\n",
    "\n",
    "train_data.drop(columns='ID', inplace=True)\n",
    "test_data.drop(columns='ID', inplace=True)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb917288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace positive with 1 and negative with 0 in the 'Sepsis' column of the train dataset\n",
    "\n",
    "train_data['Sepsis'] = train_data['Sepsis'].replace(['Positive', 'Negative'], [1, 0])\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43851e79",
   "metadata": {},
   "source": [
    "### Split the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8982f24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the X and y variables of the train dataset\n",
    "X = train_data.drop('Sepsis', axis=1)\n",
    "y = train_data['Sepsis']\n",
    "\n",
    "# Split train dataset into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Print the shape of the train dataset\n",
    "print(\"Train set shape:\", X_train.shape, y_train.shape, X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2ab14b",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "Feature scaling is a data preprocessing technique that involves transforming the values of features or variables in a dataset to a similar scale. This is to ensure that all features contribute equally to the training of models and to prevent features with larger values from dominating the models trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d3084d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scaler object using StandardScaler\n",
    "\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d76bffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use StandardScaler to scale the X_train\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=scaler.get_feature_names_out())\n",
    "\n",
    "# View the scaled X_train DataFrame\n",
    "X_train_scaled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f93eae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use StandardScaler to scale the X_val\n",
    "X_val_scaled = scaler.fit_transform(X_val)\n",
    "X_val_scaled_df = pd.DataFrame(X_val_scaled, columns=scaler.get_feature_names_out())\n",
    "\n",
    "# View the scaled X_val DataFrame\n",
    "X_val_scaled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039fbadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use StandardScaler to scale the test dataset\n",
    "test_data_scaled = scaler.fit_transform(test_data)\n",
    "test_data_scaled_df = pd.DataFrame(test_data_scaled, columns=scaler.get_feature_names_out())\n",
    "\n",
    "# View the scaled test DataFrame\n",
    "test_data_scaled_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc0a5aa",
   "metadata": {},
   "source": [
    "### Balancing X_train of the scaled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28635eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform oversampling using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled_df, y_train)\n",
    "\n",
    "# Perform undersampling using RandomUnderSampler\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_train_balanced, y_train_balanced = rus.fit_resample(X_train_scaled_df, y_train)\n",
    "\n",
    "# Print the class distribution before and after balancing\n",
    "print(\"Before balancing:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "print(\"After balancing:\")\n",
    "print(pd.Series(y_train_balanced).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8515d32",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cef7e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of models to train and evaluate\n",
    "models = [\n",
    "    ('Logistic Regression', LogisticRegression(random_state=42)),\n",
    "    ('Decision Tree', DecisionTreeClassifier(random_state=42)),\n",
    "    ('Random Forest', RandomForestClassifier(random_state=42)),\n",
    "    ('Gradient Boosting', GradientBoostingClassifier(random_state=42)),\n",
    "    ('Adaptive Boosting', AdaBoostClassifier(random_state=42)),\n",
    "    ('Support Vector Machine', SVC(random_state=42)),\n",
    "    ('Gaussian Naive Bayes', GaussianNB()),\n",
    "    ('K-Nearest Neighbors', KNeighborsClassifier())\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f7e915",
   "metadata": {},
   "source": [
    "### Model training and evaluation with the unbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8326ae43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the best_model and best f1 score variables\n",
    "unbal_best_model = None\n",
    "unbal_best_f1_score = 0.0\n",
    "\n",
    "# Create an empty dictionary to store the performance metrics of the models after training with unbalanced dataset\n",
    "unbal_performance_metrics = {}\n",
    "\n",
    "# Model training, evaluation and result calculation\n",
    "for model_name, model in models:\n",
    "    # Model training with unbalanced dataset\n",
    "    model.fit(X_train_scaled_df, y_train)\n",
    "    \n",
    "    # Using the models to make predictions on the validation set\n",
    "    y_pred_unbal = model.predict(X_val_scaled_df)\n",
    "    \n",
    "    # Calculate the performance metrics of the models on the balanced dataset\n",
    "    accuracy = accuracy_score(y_val, y_pred_unbal)\n",
    "    precision = precision_score(y_val, y_pred_unbal)\n",
    "    recall = recall_score(y_val, y_pred_unbal)\n",
    "    f1 = f1_score(y_val, y_pred_unbal)\n",
    "    roc_auc = roc_auc_score(y_val, y_pred_unbal)\n",
    "    \n",
    "    # Store the performance metrics results\n",
    "    unbal_performance_metrics[model_name] = {\n",
    "        'Unbal Accuracy': accuracy,\n",
    "        'Unbal Precision': precision,\n",
    "        'Unbal Recall': recall,\n",
    "        'Unbal F1 Score': f1,\n",
    "        'Unbal ROC_AUC': roc_auc\n",
    "    }\n",
    "    \n",
    "    # Check if the current model has a higher F1 score than the best one found so far\n",
    "    if f1 > unbal_best_f1_score:\n",
    "        unbal_best_model = model  # Update the best model\n",
    "        unbal_best_model_name = model_name # Update the best model name\n",
    "        unbal_best_f1_score = f1  # Update the best F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fe4ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to store the performance metrics of the models on the unbalanced dataset\n",
    "unbalanced_performance_metrics = pd.DataFrame(unbal_performance_metrics).transpose()\n",
    "    \n",
    "# Arrange the performance metrics DataFrame in descending order according to the F1 Score\n",
    "unbalanced_performance_metrics = unbalanced_performance_metrics.sort_values('Unbal F1 Score', ascending=False)\n",
    "\n",
    "# Show the performance metrics DataFrame of the models on the unbalanced dataset\n",
    "unbalanced_performance_metrics.style.set_caption('The Performance Metrics Of The Models On The Unbalanced Dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25762fa",
   "metadata": {},
   "source": [
    "Based on the f1 score of the models, Logistic Regression is the best model for the unbalanced dataset with an f1 score of 0.609756. Generally, Adaptive Boosting performed better in all the metrics except in the Precision and Recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6b61db",
   "metadata": {},
   "source": [
    "### Confusion matrix for unbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd393158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model prediction and confusion matrix computation\n",
    "for model_name, model in models:\n",
    "    # Model training with unbalanced dataset\n",
    "    model.fit(X_train_scaled_df, y_train)\n",
    "    \n",
    "    # Using the models to make predictions on the validation set\n",
    "    y_pred_unbal = model.predict(X_val_scaled_df)\n",
    "    \n",
    "    # Compute the confusion matrix\n",
    "    cm = confusion_matrix(y_val, y_pred_unbal)\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    print(f'Confusion Matrix For {model_name} On Unbalanced Dataset:\\n{cm}')\n",
    "    \n",
    "    # plot the confusion matrix\n",
    "    sns.heatmap(cm,\n",
    "            annot=True,\n",
    "            fmt='g',\n",
    "            xticklabels=['Churn','Not Churn'],\n",
    "            yticklabels=['Churn','Not Churn'])\n",
    "    plt.ylabel('Prediction',fontsize=13)\n",
    "    plt.xlabel('Actual',fontsize=13)\n",
    "    plt.title(f'Confusion Matrix For {model_name} On Unbalanced Dataset',fontsize=17)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96a6235",
   "metadata": {},
   "source": [
    "### Model training and evaluation with the balanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917e0931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary to store the performance metrics of the models after training with balanced dataset\n",
    "bal_performance_metrics = {}\n",
    "\n",
    "# Initialize the best_model and best f1 score variables\n",
    "bal_best_model = None\n",
    "bal_best_f1_score = 0.0\n",
    "\n",
    "# Model training, evaluation and result calculation\n",
    "for model_name, model in models:\n",
    "    # Model training with balanced dataset\n",
    "    model.fit(X_train_balanced, y_train_balanced)\n",
    "    \n",
    "    # Using the models to make predictions on the validation set\n",
    "    y_pred_bal = model.predict(X_val_scaled_df)\n",
    "    \n",
    "    # Calculate the performance metrics of the models on the balanced dataset\n",
    "    accuracy = accuracy_score(y_val, y_pred_bal)\n",
    "    precision = precision_score(y_val, y_pred_bal)\n",
    "    recall = recall_score(y_val, y_pred_bal)\n",
    "    f1 = f1_score(y_val, y_pred_bal)\n",
    "    roc_auc = roc_auc_score(y_val, y_pred_bal)\n",
    "    \n",
    "    # Store the performance metrics results\n",
    "    bal_performance_metrics[model_name] = {\n",
    "        'Bal Accuracy': accuracy,\n",
    "        'Bal Precision': precision,\n",
    "        'Bal Recall': recall,\n",
    "        'Bal F1 Score': f1,\n",
    "        'Bal ROC_AUC': roc_auc\n",
    "    }\n",
    "    \n",
    "    # Check if the current model has a higher F1 score than the best one found so far\n",
    "    if f1 > bal_best_f1_score:\n",
    "        bal_best_model = model  # Update the best model\n",
    "        bal_best_model_name = model_name # Update the best model name\n",
    "        bal_best_f1_score = f1  # Update the best F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b8814e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to store the performance metrics of the models on the balanced dataset\n",
    "balanced_performance_metrics = pd.DataFrame(bal_performance_metrics).transpose()\n",
    "    \n",
    "# Arrange the performance metrics DataFrame in descending order according to the F1 Score\n",
    "balanced_performance_metrics = balanced_performance_metrics.sort_values('Bal F1 Score', ascending=False)\n",
    "\n",
    "# Show the performance metrics DataFrame of the models on the balanced dataset\n",
    "balanced_performance_metrics.style.set_caption('The Performance Metrics Of The Models On The Balanced Dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747cce02",
   "metadata": {},
   "source": [
    "Based on the f1 score of the models, Adaptive Boosting is the best model for the balanced dataset with an f1 score of 0.681319. Generally, Adaptive Boosting performed better in all the metrics except in the Recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8335bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model prediction and confusion matrix computation\n",
    "for model_name, model in models:\n",
    "    # Model training with balanced dataset\n",
    "    model.fit(X_train_balanced, y_train_balanced)\n",
    "    \n",
    "    # Using the models to make predictions on the validation set\n",
    "    y_pred_bal = model.predict(X_val_scaled_df)\n",
    "    \n",
    "    # Compute the confusion matrix\n",
    "    cm = confusion_matrix(y_val, y_pred_bal)\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    print(f'Confusion Matrix For {model_name} On Balanced Dataset:\\n{cm}')\n",
    "    \n",
    "    # plot the confusion matrix\n",
    "    sns.heatmap(cm,\n",
    "            annot=True,\n",
    "            fmt='g',\n",
    "            xticklabels=['Positive','Negative'],\n",
    "            yticklabels=['Positive','Negative'])\n",
    "    plt.ylabel('Prediction',fontsize=13)\n",
    "    plt.xlabel('Actual',fontsize=13)\n",
    "    plt.title(f'Confusion Matrix For {model_name} On Balanced Dataset',fontsize=17)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a20575e",
   "metadata": {},
   "source": [
    "### Hyper-parameter Tuning\n",
    "\n",
    "Hyperparameters are adjustable parameters whose values control the model training process.\n",
    "\n",
    "Hyperparameter tuning (or hyperparameter optimization) is a process used to determine the right combination of hyperparameters that maximizes the model performance. It works by running multiple trials in a single training process. The hyperparameters are set within specified limits and executed to identify the set of hyperparameter values that are best suited for a model to give optimal results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a778c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the available parameters for each model\n",
    "\n",
    "for model_name, model in models:\n",
    "    available_params = model.get_params()\n",
    "    print(f'Available Parameters For {model_name}:{available_params}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f39d417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary to store the performance metrics of the tuned models on the balanced dataset\n",
    "tun_bal_performance_metrics = {}\n",
    "\n",
    "# Initialize the best_model and best f1 score variables\n",
    "tun_bal_best_model = None\n",
    "tun_bal_best_f1_score = 0.0\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "for model_name, model in models:\n",
    "    params_selection = {\n",
    "        'Logistic Regression' : {'solver': ['newton-cg', 'lbfgs', 'liblinear'], 'C': [10, 1.0, 0.01]},\n",
    "        'Decision Tree' : {'max_depth': [1, 3], 'min_samples_split': [1, 2, 5, 10], 'min_samples_leaf': [0.5, 1, 2]},\n",
    "        'Random Forest' : {'n_estimators': [500, 700], 'max_depth': [1, 3]},\n",
    "        'Gradient Boosting' : {'n_estimators': [100, 150],'learning_rate': [0.1, 1.0, 5.0]},\n",
    "        'Adaptive Boosting' : {'n_estimators': [10, 50, 100],'learning_rate': [0.1, 1.0, 5.0]},\n",
    "        'Support Vector Machine' : {'kernel': ['poly', 'rbf', 'sigmoid'], 'C': [0.5, 1.0, 10]},\n",
    "        'Gaussian Naive Bayes' : {'var_smoothing': [float, 1e-09]},\n",
    "        'K-Nearest Neighbors' : {'weights': ['uniform', 'distance'], 'metric': ['euclidean', 'manhattan', 'minkowski']}\n",
    "    }\n",
    "   \n",
    "    # Get the selected parameter values for the models to tune\n",
    "    param_grid = params_selection[model_name]\n",
    "    \n",
    "    # Perform hyperparameter tuning using GridSearchCV\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1', verbose=0, refit=True)\n",
    "    grid_search.fit(X_train_balanced, y_train_balanced)\n",
    "         \n",
    "    # Get the best of each model with the best parameters\n",
    "    best_params = grid_search.best_params_\n",
    "    best_params_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Show the best parameters for each model\n",
    "    print(f'The best parameters for {model_name} are {best_params}\\n')\n",
    "    \n",
    "    # Using each model with it's best parameters to make predictions on the validation set\n",
    "    best_params_model.fit(X_train_balanced, y_train_balanced)\n",
    "    y_pred_bal_tun = best_params_model.predict(X_val)\n",
    "    \n",
    "    # Calculate the performance metrics on the balanced dataset for each model with it's best parameters\n",
    "    accuracy = accuracy_score(y_val, y_pred_bal_tun)\n",
    "    precision = precision_score(y_val, y_pred_bal_tun)\n",
    "    recall = recall_score(y_val, y_pred_bal_tun)\n",
    "    f1 = f1_score(y_val, y_pred_bal_tun)\n",
    "    roc_auc = roc_auc_score(y_val, y_pred_bal_tun)\n",
    "    \n",
    "    # Store the performance metrics results\n",
    "    tun_bal_performance_metrics[model_name] = {\n",
    "        'Tuned-Bal Accuracy': accuracy,\n",
    "        'Tuned-Bal Precision': precision,\n",
    "        'Tuned-Bal Recall': recall,\n",
    "        'Tuned-Bal F1 Score': f1,\n",
    "        'Tuned-Bal ROC_AUC': roc_auc\n",
    "    }\n",
    "    \n",
    "    # Check if the current model has a higher F1 score than the best one found so far\n",
    "    if f1 > tun_bal_best_f1_score:\n",
    "        tun_bal_best_model = model  # Update the best model\n",
    "        tun_bal_best_model_name = model_name # Update the best model name\n",
    "        tun_bal_best_f1_score = f1  # Update the best F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51a0d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to store the performance metrics of the tuned models on the balanced dataset\n",
    "tuned_bal_performance_metrics = pd.DataFrame(tun_bal_performance_metrics).transpose()\n",
    "    \n",
    "# Arrange the performance metrics DataFrame in descending order according to the F1 Score\n",
    "tuned_bal_performance_metrics = tuned_bal_performance_metrics.sort_values('Tuned-Bal F1 Score', ascending=False)\n",
    "\n",
    "# Show the performance metrics DataFrame of the tuned models on the balanced dataset\n",
    "tuned_bal_performance_metrics.style.set_caption('The Performance Metrics Of The Tuned Models On The Balanced Dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a5bbf4",
   "metadata": {},
   "source": [
    "From the table above, it is observed that the models did not perform better after hyper-parametr tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701a629c",
   "metadata": {},
   "source": [
    "# Combine the performance metrics of the models\n",
    "\n",
    "The performance metrics of the models obtained for the unbalanced dataset, the balanced dataset and the performance metrics obtained for the tuned models will be combined together respectively to easily evaluate how each model performed in the three conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14978b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the DataFrames while preserving columns\n",
    "combined_performance_metrics = pd.concat([unbalanced_performance_metrics, balanced_performance_metrics,\n",
    "                                          tuned_bal_performance_metrics], axis=1)\n",
    "\n",
    "# Arrange the combined performance metrics DataFrame in descending order according to the F1 Score of the tuned models\n",
    "combined_performance_metrics = combined_performance_metrics.sort_values('Bal F1 Score', ascending=False)\n",
    "\n",
    "# Show the performance metrics DataFrame of the tuned models on the balanced dataset\n",
    "combined_performance_metrics.style.set_caption('The Combined Performance Metrics Of The Models')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e759a9",
   "metadata": {},
   "source": [
    "As shown in the table above which combines all the evaluation metrics, Adaptive Boosting is the best model after the entire modelling process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1176082c",
   "metadata": {},
   "source": [
    "### Exportation\n",
    "\n",
    "The key Machine Learning objects, including the best model, will be exported and used later to develop a FastAPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cb3ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the best model_name with the highest F1 score\n",
    "\n",
    "print(f\"The best model based on F1 score is {bal_best_model_name} and it's F1 score is {bal_best_f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dafe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model_name with the highest F1 score\n",
    "\n",
    "best_model = bal_best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f17b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store all the Machine Learning components\n",
    "\n",
    "model_components = {\n",
    "    'model': best_model,\n",
    "    'scaler': scaler\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6cd7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an export folder named 'export'\n",
    "\n",
    "!mkdir export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4962ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a path to the export folder\n",
    "\n",
    "destination = os.path.join('.', 'export')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53781af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the components to a file using pickle\n",
    "\n",
    "with open (os.path.join(destination, 'ml_components.pkl'), 'wb') as f:\n",
    "    pickle.dump(model_components, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7239cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create requirements.txt file in export folder to describe the virtual environment used for the Machine Learning processes\n",
    "\n",
    "!pip freeze > export/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f130db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip the export folder and name the zipped export folder as 'export.zip'\n",
    "# !tar -a -c -f export.zip export\n",
    "\n",
    "# Delete the original export folder, leaving only the zipped export folder\n",
    "# !rmdir /s /q export"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
